{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Modeling Betcounts per Wager\n",
    "\n",
    "In this notebook I will focus on finding a model to predict the total number of bets made per each individual wager. Through this process a production model will be selected and compared to the baseline metrics to understand how strong the model is performing. I will then analyze the most influential features contributing to the models predictions which will tell the story about what is contributing to the total number of bets that were made through the first five months of the 2018 horse racing season."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/nelson/anaconda3/lib/python3.6/site-packages/sklearn/ensemble/weight_boosting.py:29: DeprecationWarning: numpy.core.umath_tests is an internal NumPy module and should not be imported. It will be removed in a future NumPy release.\n",
      "  from numpy.core.umath_tests import inner1d\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import seaborn as sns\n",
    "import matplotlib.pyplot as plt\n",
    "import time\n",
    "import pickle\n",
    "import csv\n",
    "\n",
    "from sklearn.model_selection import train_test_split, GridSearchCV\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.linear_model import Lasso, LinearRegression, Ridge, ElasticNet\n",
    "from sklearn.ensemble import RandomForestRegressor, GradientBoostingRegressor, AdaBoostRegressor\n",
    "from sklearn.svm import SVR\n",
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn.metrics import r2_score, mean_squared_error\n",
    "\n",
    "%matplotlib inline\n",
    "\n",
    "plt.style.use('fivethirtyeight')\n",
    "\n",
    "np.random.seed(42)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Load the Data\n",
    "\n",
    "Loading in the merged dataframe created during EDA which includes the data for each wager made along with the customer demographic data for each individual that made the wager."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "final_wager_df = pd.read_csv('../data/final_wager.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>race_month</th>\n",
       "      <th>userid</th>\n",
       "      <th>address</th>\n",
       "      <th>gender</th>\n",
       "      <th>age</th>\n",
       "      <th>internet_/_shop</th>\n",
       "      <th>bet_type_category</th>\n",
       "      <th>handle</th>\n",
       "      <th>revenue</th>\n",
       "      <th>betcount</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>2018-01-01</td>\n",
       "      <td>199</td>\n",
       "      <td>NY</td>\n",
       "      <td>Male</td>\n",
       "      <td>18</td>\n",
       "      <td>Mobile</td>\n",
       "      <td>WPS</td>\n",
       "      <td>200.0</td>\n",
       "      <td>20.0</td>\n",
       "      <td>5.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>2018-01-01</td>\n",
       "      <td>199</td>\n",
       "      <td>NY</td>\n",
       "      <td>Male</td>\n",
       "      <td>18</td>\n",
       "      <td>Desktop</td>\n",
       "      <td>WPS</td>\n",
       "      <td>25.0</td>\n",
       "      <td>2.5</td>\n",
       "      <td>1.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>2018-02-01</td>\n",
       "      <td>199</td>\n",
       "      <td>NY</td>\n",
       "      <td>Male</td>\n",
       "      <td>18</td>\n",
       "      <td>Mobile</td>\n",
       "      <td>WPS</td>\n",
       "      <td>42.0</td>\n",
       "      <td>4.2</td>\n",
       "      <td>1.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>2018-01-01</td>\n",
       "      <td>531</td>\n",
       "      <td>NY</td>\n",
       "      <td>Female</td>\n",
       "      <td>18</td>\n",
       "      <td>Mobile</td>\n",
       "      <td>WPS</td>\n",
       "      <td>205.0</td>\n",
       "      <td>20.5</td>\n",
       "      <td>1.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>2018-01-01</td>\n",
       "      <td>887</td>\n",
       "      <td>NY</td>\n",
       "      <td>Male</td>\n",
       "      <td>18</td>\n",
       "      <td>Mobile</td>\n",
       "      <td>WPS</td>\n",
       "      <td>205.0</td>\n",
       "      <td>20.5</td>\n",
       "      <td>2.0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   race_month  userid address  gender  age internet_/_shop bet_type_category  \\\n",
       "0  2018-01-01     199      NY    Male   18          Mobile               WPS   \n",
       "1  2018-01-01     199      NY    Male   18         Desktop               WPS   \n",
       "2  2018-02-01     199      NY    Male   18          Mobile               WPS   \n",
       "3  2018-01-01     531      NY  Female   18          Mobile               WPS   \n",
       "4  2018-01-01     887      NY    Male   18          Mobile               WPS   \n",
       "\n",
       "   handle  revenue  betcount  \n",
       "0   200.0     20.0       5.0  \n",
       "1    25.0      2.5       1.0  \n",
       "2    42.0      4.2       1.0  \n",
       "3   205.0     20.5       1.0  \n",
       "4   205.0     20.5       2.0  "
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "final_wager_df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "race_month            object\n",
       "userid                 int64\n",
       "address               object\n",
       "gender                object\n",
       "age                    int64\n",
       "internet_/_shop       object\n",
       "bet_type_category     object\n",
       "handle               float64\n",
       "revenue              float64\n",
       "betcount             float64\n",
       "dtype: object"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "final_wager_df.dtypes"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Setup Dummy Variables and Modeling Dataframe\n",
    "\n",
    "With the final dataset now completed we need to ensure that all of our data columns are in a numerical data type. This means casting our categorical features as dummies so that they are recoginized as numeric data types to ensure our regression model can make predictions. With pandas we use \"get_dummies\" on the dataframe to cast all object columns to boolean numerical type columns."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "model_df = pd.get_dummies(final_wager_df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>userid</th>\n",
       "      <th>age</th>\n",
       "      <th>handle</th>\n",
       "      <th>revenue</th>\n",
       "      <th>betcount</th>\n",
       "      <th>race_month_2018-01-01</th>\n",
       "      <th>race_month_2018-02-01</th>\n",
       "      <th>race_month_2018-03-01</th>\n",
       "      <th>race_month_2018-04-01</th>\n",
       "      <th>race_month_2018-05-01</th>\n",
       "      <th>address_CA</th>\n",
       "      <th>address_FL</th>\n",
       "      <th>address_KY</th>\n",
       "      <th>address_NY</th>\n",
       "      <th>gender_Female</th>\n",
       "      <th>gender_Male</th>\n",
       "      <th>internet_/_shop_Desktop</th>\n",
       "      <th>internet_/_shop_Mobile</th>\n",
       "      <th>bet_type_category_Exotic</th>\n",
       "      <th>bet_type_category_WPS</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>199</td>\n",
       "      <td>18</td>\n",
       "      <td>200.0</td>\n",
       "      <td>20.0</td>\n",
       "      <td>5.0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>199</td>\n",
       "      <td>18</td>\n",
       "      <td>25.0</td>\n",
       "      <td>2.5</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>199</td>\n",
       "      <td>18</td>\n",
       "      <td>42.0</td>\n",
       "      <td>4.2</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>531</td>\n",
       "      <td>18</td>\n",
       "      <td>205.0</td>\n",
       "      <td>20.5</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>887</td>\n",
       "      <td>18</td>\n",
       "      <td>205.0</td>\n",
       "      <td>20.5</td>\n",
       "      <td>2.0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   userid  age  handle  revenue  betcount  race_month_2018-01-01  \\\n",
       "0     199   18   200.0     20.0       5.0                      1   \n",
       "1     199   18    25.0      2.5       1.0                      1   \n",
       "2     199   18    42.0      4.2       1.0                      0   \n",
       "3     531   18   205.0     20.5       1.0                      1   \n",
       "4     887   18   205.0     20.5       2.0                      1   \n",
       "\n",
       "   race_month_2018-02-01  race_month_2018-03-01  race_month_2018-04-01  \\\n",
       "0                      0                      0                      0   \n",
       "1                      0                      0                      0   \n",
       "2                      1                      0                      0   \n",
       "3                      0                      0                      0   \n",
       "4                      0                      0                      0   \n",
       "\n",
       "   race_month_2018-05-01  address_CA  address_FL  address_KY  address_NY  \\\n",
       "0                      0           0           0           0           1   \n",
       "1                      0           0           0           0           1   \n",
       "2                      0           0           0           0           1   \n",
       "3                      0           0           0           0           1   \n",
       "4                      0           0           0           0           1   \n",
       "\n",
       "   gender_Female  gender_Male  internet_/_shop_Desktop  \\\n",
       "0              0            1                        0   \n",
       "1              0            1                        1   \n",
       "2              0            1                        0   \n",
       "3              1            0                        0   \n",
       "4              0            1                        0   \n",
       "\n",
       "   internet_/_shop_Mobile  bet_type_category_Exotic  bet_type_category_WPS  \n",
       "0                       1                         0                      1  \n",
       "1                       0                         0                      1  \n",
       "2                       1                         0                      1  \n",
       "3                       1                         0                      1  \n",
       "4                       1                         0                      1  "
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model_df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "model_df.drop('userid', axis=1, inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(34904, 19)"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model_df.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Save the Data\n",
    "\n",
    "Saving a final copy of the modeling dataframe."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "model_df.to_csv('../data/model_df.csv')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Setup X and y\n",
    "\n",
    "With our final dataframe now completed we need to identify both the feature dataframe which will be used to train our model (known as X) and the target variable that we are predicting (known as y).\n",
    "- X : will be all features in the data provided minus the betcount\n",
    "- y : will be just the betcount values as this is what we are trying to predict per each wager made"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "X = model_df.drop('betcount', axis=1) \n",
    "y = model_df['betcount']"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Train / Test / Split\n",
    "\n",
    "To create a batch of data for both training our model and then testing our model we perfom a train, test, and split on our identified X and y variables. This is created at random.\n",
    "- I will use the default split of 75% training data and 25% testing data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train, X_test, y_train, y_test = train_test_split(X, y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "((26178, 18), (26178,))"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X_train.shape, y_train.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "((8726, 18), (8726,))"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X_test.shape, y_test.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Save the Data\n",
    "\n",
    "Saving the train and test split data in order for future use."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train.to_csv('../data/X_train.csv', index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_test.to_csv('../data/X_test.csv', index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open('../pickles/y_train.pkl', 'wb+') as f:\n",
    "    pickle.dump(y_train, f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open('../pickles/y_test.pkl', 'wb+') as f:\n",
    "    pickle.dump(y_test, f)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Scale the Data\n",
    "\n",
    "To ensure the model can make accurate predictions on the target variable we need each feature to be placed on the same scale so that features are not over weighted unjustly."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "ss = StandardScaler()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train_sc = ss.fit_transform(X_train)\n",
    "X_test_sc = ss.transform(X_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Save the Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open('../pickles/standard_scaler.pkl', 'wb+') as f:\n",
    "    pickle.dump(ss, f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open('../data/X_train_sc.csv', 'w+') as f:\n",
    "    csv_writer = csv.writer(f)\n",
    "    csv_writer.writerows(X_train_sc)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open('../data/X_test_sc.csv', 'w+') as f:\n",
    "    csv_writer = csv.writer(f)\n",
    "    csv_writer.writerows(X_test_sc)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Modeling Function\n",
    "\n",
    "To ensure efficiency in the modeling process I have created a function to assist me. \n",
    "\n",
    "##### Explanation of Function\n",
    "\n",
    "1. **Modeling Function:** All of my models are going to utilize grid searching in order to find the optimal hyper-parameters to use within the model for predictions, so I will instantiate GridSearchCV within the function as gs. Next, the function will call upon the fit method in oder to fit the necessary data that will be used to train the model and ultimately make predictions. Upon completion of the fitting a saved pickle of the model will be saved with the key of the dictionary from the corresponding pipeline (model abbreviation) and the time stamp (in seconds) to be able to uniquely identify which model the pickle belongs.\n",
    "    - The following arguments will be required to run this function:\n",
    "        1. pipe : each of my models will have a pipeline setup prior to fitting a model\n",
    "        2. params : each of my models will have hyper-parameters setup for the identified model in the pipeline to be used for grid searching for the best hyper-parameters\n",
    "        3. X_train : the features from the data used for training the model during fit\n",
    "        4. y_train : the target varibale values (total bets per wager) to be used for training the model during fit"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "def modeling_func(pipe, params, X_train, y_train, cv=3):\n",
    "    gs = GridSearchCV(pipe, param_grid=params, cv=cv)\n",
    "    gs.fit(X_train, y_train)\n",
    "    with open(f'../pickles/{\"_\".join(pipe.named_steps.keys())}{int(time.time())}.pkl', 'wb+') as f:\n",
    "        pickle.dump(gs, f)\n",
    "    return gs"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Baseline Model and Metrics\n",
    "\n",
    "#### Average number of bets made per wager\n",
    "\n",
    "To get a baseline for the target variable (number of bets made per wager) I will take the average of the y_train and y_test variables which contain the actual total number of bets made with each wager so that I have an idea of what the average of my model predictions should be targeting. This is known as the naive model.\n",
    "\n",
    "#### R2 Score and Root Mean Squared Error\n",
    "\n",
    "Lets also understand some baseline metrics in the target variable. These metrics will provide some insights into what my model should be targeting or performing better than:\n",
    "- **R2:** Is the explained variation in the target variable from the features provided in the model. In other words the variation in the model from the line of best fit. This will be explained as a percentage and we'll want to target a value closer to one.\n",
    "- **RMSE (Root Mean Square Error):** Is the average distance from the line of best fit. This can be interpreted in the same units as the target variable (number of total bets). This value should be lower as we want the average distance from our line of best fit to be small.\n",
    "\n",
    "##### Interpretation of Baseline Model and Metrics\n",
    "\n",
    "The average number of bets being made in the train and test batches are almost identical at around 7.5 bets made per wager so when we make the final predictions from the production model we'll get the mean and compare how close it is to 7.5 bets.\n",
    "\n",
    "The R2 score on both the train and test models against the baseline predictions (total bets per wager) is giving a baseline variance of 0 meaning that the baseline R2 is the worst model possible. This means that if our model predictions are explaning any variance better than zero we are already in a better spot than the baseline. I will be optimizing for an R2 as close to one as possible.\n",
    "\n",
    "The RMSE score on both the train and test models against the baseline actuals (total bets per wager) is very similar at around 18.24 and 19.79. Meaning that on average the baseline model is off by 18.24 total bets from the line of best fit. The goal will be to optimize my model to obtian a RMSE lower than 19.79."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Average Bets Made Train Data: 7.429559171823668\n",
      "Average Bets Made Test Data: 7.6410726564290625\n"
     ]
    }
   ],
   "source": [
    "print(f'Average Bets Made Train Data: {y_train.mean()}')\n",
    "print(f'Average Bets Made Test Data: {y_test.mean()}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "R2 score Train Data: 0.0\n",
      "R2 score Test Data: 0.0\n"
     ]
    }
   ],
   "source": [
    "print(f'R2 score Train Data: {r2_score(y_train, [y_train.mean()] * len(y_train))}')\n",
    "print(f'R2 score Test Data: {r2_score(y_test, [y_test.mean()] * len(y_test))}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "RMSE score Train Data: 18.248556172903967\n",
      "RMSE score Test Data: 19.79890837425836\n"
     ]
    }
   ],
   "source": [
    "print(f'RMSE score Train Data: {np.sqrt(mean_squared_error(y_train, [y_train.mean()] * len(y_train)))}')\n",
    "print(f'RMSE score Test Data: {np.sqrt(mean_squared_error(y_test, [y_test.mean()] * len(y_test)))}')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### GridSearch with ElasticNet\n",
    "\n",
    "#### GridSearchCV\n",
    "\n",
    "GridSearchCV is a technique that searches for the optimal hyper-parameters provided during the instantiating of the GridSearchCV model. Using its built in cross validation it can search over the grid of the provided hyperparameters to evaluate the performance of each and then use the parameter(s) it found to be the best when making the predictions.\n",
    "\n",
    "\n",
    "#### ElasticNet\n",
    "\n",
    "The elastic net regularization technique imposes the lasso and ridge penalties (L1 and L2) on the estimates to those that were identified as performing the worst in the model. In this technique the elastic net will choose the optimal method (Lasso or Ridge) if one is found to be more optimal than the other."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**NOTE:** The below line of code can be run to use the same pickled model that I used during my modeling process"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {},
   "outputs": [],
   "source": [
    "# gs = pd.read_pickle('../pickles/enet1542340254.pkl')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Pipeline and Parameters\n",
    "\n",
    "Pipelines are the sklearn method that take in a sequential list of steps that end with the appropriate estimator or model that you are planning to run.\n",
    "\n",
    "Parameters should be a dictionary with the keys referencing the different steps and parameters you are looking to tune followed by the values for that parameter that you are looking to tune through the search."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [],
   "source": [
    "pipe = Pipeline([\n",
    "    ('enet', ElasticNet())\n",
    "])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [],
   "source": [
    "params = {\n",
    "    'enet__alpha': np.logspace(-3,3,9),\n",
    "    'enet__l1_ratio': [2.5, 3, 3.5, 4, 4.5]\n",
    "}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### ElasticNet Model\n",
    "\n",
    "**NOTE:** Returns the coefficient of determination R2 of the prediction.\n",
    "\n",
    "The train and test batches with the ElasticNet model is scoring as follows:\n",
    "- Train R2: 0.0\n",
    "- Test R2: -0.0001\n",
    "\n",
    "##### Interpretation\n",
    "\n",
    "The model score on the test data is negative indicating that this dataset is not well suited for linear models. These scores are not beating the baseline metrics and thus will not be the model I will choose for this data. As we saw with the correlation heat map earlier there wasn't any strong linera connections to the total betcounts per wager so this makes sense that the ElasticNet liner model is not perfoming well."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**NOTE:** To run a model uncomment the below line of code"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# gs = modeling_func(pipe, params, X_train_sc, y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'enet__alpha': 0.1778279410038923, 'enet__l1_ratio': 3}"
      ]
     },
     "execution_count": 59,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "gs.best_params_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "-0.00011412837368274253"
      ]
     },
     "execution_count": 60,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "gs.score(X_test_sc, y_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.0"
      ]
     },
     "execution_count": 61,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "gs.score(X_train_sc, y_train)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### GridSeach with GradientBoostRegressor\n",
    "\n",
    "#### GridSearchCV\n",
    "\n",
    "GridSearchCV is a technique that searches for the optimal hyper-parameters provided during the instantiating of the GridSearchCV model. Using its built in cross validation it can search over the grid of the provided hyperparameters to evaluate the performance of each and then use the parameter(s) it found to be the best when making the predictions.\n",
    "\n",
    "#### Gradient Boost\n",
    "\n",
    "When boosting a model the model is building multiple simple models and learning from these models to be more approximate when predicting. These simple models are referred to as weak models or weak learners. \n",
    "\n",
    "Gradient Boosting looks at these weak models sequentially and trains on the residuals or errors in order to give more importance to the less accurate predictions and once completed uses what was learned from these predictions to combine with the strong predictions to have a better overall approximation."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**NOTE:** The below line of code can be run to use the same pickled model that I used during my modeling process"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# gs = pd.read_pickle('../pickles/gb1542340446.pkl')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Pipeline and Parameters\n",
    "\n",
    "Pipelines are the sklearn method that take in a sequential list of steps that end with the appropriate estimator or model that you are planning to run.\n",
    "\n",
    "Parameters should be a dictionary with the keys referencing the different steps and parameters you are looking to tune followed by the values for that parameter that you are looking to tune through the search."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [],
   "source": [
    "pipe = Pipeline([\n",
    "    ('gb', GradientBoostingRegressor())\n",
    "])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [],
   "source": [
    "params_gb = {\n",
    "    'gb__n_estimators':[200, 210, 220],\n",
    "    'gb__max_depth':[3, 5, 7]\n",
    "}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Gradient Boost Model\n",
    "\n",
    "**NOTE:** Returns the coefficient of determination R2 of the prediction.\n",
    "\n",
    "The train and test batches with the GradietnBoost model is scoring as follows:\n",
    "- Train R2 Score: 0.7851\n",
    "- Test R2 Score: 0.3825\n",
    "\n",
    "##### Interpretation\n",
    "\n",
    "The train score is now explaining around 78.51% of the variance in the predictions from the features in the dataset along with the test explaining around 38.25% of the variance. This model is definitely overfit, as the training score is greater than the testing score, but this model is out performing the baseline model and is the best we've seen so far."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**NOTE:** To run a model uncomment the below line of code"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [],
   "source": [
    "# gs = modeling_func(pipe, params_gb, X_train_sc, y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'gb__max_depth': 5, 'gb__n_estimators': 200}"
      ]
     },
     "execution_count": 53,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "gs.best_params_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.7851044357317457"
      ]
     },
     "execution_count": 54,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "gs.score(X_train_sc, y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.3825093662132626"
      ]
     },
     "execution_count": 55,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "gs.score(X_test_sc, y_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### GridSearch with AdaBoostRegressor\n",
    "\n",
    "#### GridSearchCV\n",
    "\n",
    "GridSearchCV is a technique that searches for the optimal hyper-parameters provided during the instantiating of the GridSearchCV model. Using its built in cross validation it can search over the grid of the provided hyperparameters to evaluate the performance of each and then use the parameter(s) it found to be the best when making the predictions.\n",
    "\n",
    "#### AdaBoost\n",
    "\n",
    "Again, boosting a model is the process of building multiple simple models and learning from these models to be more approximate when predicting. These simple models are referred to as weak models or weak learners.\n",
    "\n",
    "AdaBoost will work similarily to Gradient Boost in that it looks at these weak learners to train itself except it will modify the weights attached to the less accurate predictions and then combine what its learned back to the stronger predictions to make a better overall approximation."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**NOTE:** The below line of code can be run to use the same pickled model that I used during my modeling process"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# gs = pd.read_pickle('../pickles/ada1542340860.pkl')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Pipeline and Parameters\n",
    "\n",
    "Pipelines are the sklearn method that take in a sequential list of steps that end with the appropriate estimator or model that you are planning to run.\n",
    "\n",
    "Parameters should be a dictionary with the keys referencing the different steps and parameters you are looking to tune followed by the values for that parameter that you are looking to tune through the search."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "metadata": {},
   "outputs": [],
   "source": [
    "pipe = Pipeline([\n",
    "    ('ada', AdaBoostRegressor())\n",
    "])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "metadata": {},
   "outputs": [],
   "source": [
    "params_ada = {\n",
    "    'ada__n_estimators':[120, 140, 160]\n",
    "}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### AdaBoost Model\n",
    "\n",
    "**NOTE:** Returns the coefficient of determination R2 of the prediction.\n",
    "\n",
    "The train and test batches with the AdaBoost model is scoring as follows:\n",
    "- Train R2 Score: -0.0232\n",
    "- Test R2 Score: 0.0090\n",
    "\n",
    "##### Interpretation\n",
    "\n",
    "The model scores have returned to the lowest we've seen when doing the ElasticNet modeling which means we are not performing much better than the baseline model and thus will not use this model."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**NOTE:** To run a model uncomment the below line of code"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "metadata": {},
   "outputs": [],
   "source": [
    "# gs = modeling_func(pipe, params_ada, X_train_sc, y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'ada__n_estimators': 140}"
      ]
     },
     "execution_count": 67,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "gs.best_params_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "-0.023289600567534485"
      ]
     },
     "execution_count": 68,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "gs.score(X_train_sc, y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.009018999396452787"
      ]
     },
     "execution_count": 69,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "gs.score(X_test_sc, y_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### GridSearch with RandomForestRegressor\n",
    "\n",
    "#### GridSearchCV\n",
    "\n",
    "GridSearchCV is a technique that searches for the optimal hyper-parameters provided during the instantiating of the GridSearchCV model. Using its built in cross validation it can search over the grid of the provided hyperparameters to evaluate the performance of each and then use the parameter(s) it found to be the best when making the predictions.\n",
    "\n",
    "#### RandomForest\n",
    "\n",
    "This ensemble modeling technique will create decision trees from a random subset of features in the dataset and use averaging on those trees to improve the accuracy of the model."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**NOTE:** The below line of code can be run to use the same pickled model that I used during my modeling process"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# gs_rf = pd.read('../pickles/rf1542341132.pkl')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Pipeline and Parameters\n",
    "\n",
    "Pipelines are the sklearn method that take in a sequential list of steps that end with the appropriate estimator or model that you are planning to run.\n",
    "\n",
    "Parameters should be a dictionary with the keys referencing the different steps and parameters you are looking to tune followed by the values for that parameter that you are looking to tune through the search."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "metadata": {},
   "outputs": [],
   "source": [
    "pipe = Pipeline([\n",
    "    ('rf', RandomForestRegressor())\n",
    "])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "metadata": {},
   "outputs": [],
   "source": [
    "params_rf = {\n",
    "    'rf__n_estimators':[140, 150, 160, 170]\n",
    "}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Random Forest Model\n",
    "\n",
    "**NOTE:** Returns the coefficient of determination R2 of the prediction.\n",
    "\n",
    "The train and test batches with the Random Forest model is scoring as follows:\n",
    "- Train Accuracy Score: 0.9017\n",
    "- Test Accuracy Score: 0.3189\n",
    "\n",
    "##### Interpretation\n",
    "\n",
    "This model is showing an R2 of 0.9017 on the train data which means that 90.17% of the variance in our predictions against the actual values (total bets per wager) can be explained by the features in model and the R2 score of 0.3189 on the test data means that 31.89% of the variance in our predictions against the actual values (total bet per wager) can be explained by the features in the model. The Random Forest model has the highest R2 score on the train data that we've seen so far but ideally we are focused on obtaining the best score on the test data as this is the unseen data we are making predictions from so it's close but doesn't beat the score obtained from the Gradient Boost model."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**NOTE:** To run a model uncomment the below line of code"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "metadata": {},
   "outputs": [],
   "source": [
    "# gs = modeling_func(pipe, params_rf, X_train, y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'rf__n_estimators': 150}"
      ]
     },
     "execution_count": 73,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "gs.best_params_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 74,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.9017104271150153"
      ]
     },
     "execution_count": 74,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "gs.score(X_train, y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 75,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.31893094456550153"
      ]
     },
     "execution_count": 75,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "gs.score(X_test, y_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### GridSearch with SupportVectorRegressor\n",
    "\n",
    "#### GridSearchCV\n",
    "\n",
    "GridSearchCV is a technique that searches for the optimal hyper-parameters provided during the instantiating of the GridSearchCV model. Using its built in cross validation it can search over the grid of the provided hyperparameters to evaluate the performance of each and then use the parameter(s) it found to be the best when making the predictions.\n",
    "\n",
    "#### SupportVectorRegressor\n",
    "\n",
    "Using the training data the support vector regressor will setup two categories and determine how the data should be categorized. When applied to the unseen testing data the model will categorize the data into these identified categories. The model learns from these categories and then builds the optimal model."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**NOTE:** The below line of code can be run to use the same pickled model that I used during my modeling process"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "gs_svr = pd.read_pickle('../pickles/svr1542325470.pkl')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Pipeline and Parameters\n",
    "\n",
    "Pipelines are the sklearn method that take in a sequential list of steps that end with the appropriate estimator or model that you are planning to run.\n",
    "\n",
    "Parameters should be a dictionary with the keys referencing the different steps and parameters you are looking to tune followed by the values for that parameter that you are looking to tune through the search."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 76,
   "metadata": {},
   "outputs": [],
   "source": [
    "pipe = Pipeline([\n",
    "    ('svr', SVR())\n",
    "])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 77,
   "metadata": {},
   "outputs": [],
   "source": [
    "params_svr = {\n",
    "    'svr__C':[.05, .5, 1],\n",
    "    'svr__epsilon':[.01, .1, .5]\n",
    "}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Support Vector Regressor Model\n",
    "\n",
    "**NOTE:** Returns the coefficient of determination R2 of the prediction.\n",
    "\n",
    "The train and test batches with the Support Vector model is scoring as follows:\n",
    "- Train Accuracy Score: 0.1185\n",
    "- Test Accuracy Score: 0.1019\n",
    "\n",
    "##### Interpretation\n",
    "\n",
    "This model is showing an R2 of 0.1185 on the train data which means that 11.85% of the variance in our predictions against the actual values (total bets per wager) can be explained by the features in model and the R2 score of 0.1019 on the test data means that 10.19% of the variance in our predictions against the actual values (total bet per wager) can be explained by the features in the model. The model is definitely performing better than the baseline and better than both the AdaBoost ensemble model and ElasticNet linear model but didn't out perform two of the ensemble models, RandomForest and GradientBoost."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**NOTE:** To run a model uncomment the below line of code"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 78,
   "metadata": {},
   "outputs": [],
   "source": [
    "gs = modeling_func(pipe, params_svr, X_train_sc, y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 79,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'svr__C': 1, 'svr__epsilon': 0.5}"
      ]
     },
     "execution_count": 79,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "gs.best_params_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 80,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.11859259968811098"
      ]
     },
     "execution_count": 80,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "gs.score(X_train_sc, y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 81,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.10199837095709108"
      ]
     },
     "execution_count": 81,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "gs.score(X_test_sc, y_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Moving to 03-Production_Model"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
